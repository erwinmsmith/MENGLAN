{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c41fcd-c416-4153-8b2c-1c8170b9ee73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:29.216749Z",
     "iopub.status.busy": "2024-08-08T05:52:29.216377Z",
     "iopub.status.idle": "2024-08-08T05:52:32.702136Z",
     "shell.execute_reply": "2024-08-08T05:52:32.701560Z",
     "shell.execute_reply.started": "2024-08-08T05:52:29.216724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:52:29.429470: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-08 13:52:29.469822: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-08 13:52:30.236879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam ,SGD\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten ,Reshape ,Bidirectional,Activation, Add,Layer,add,GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, BatchNormalization, Reshape\n",
    "from keras import backend as K\n",
    "import os\n",
    "from keras.metrics import MeanAbsoluteError, MeanSquaredError\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import Input, MultiHeadAttention, LSTM, Dense, Flatten, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # 导入优化器模块\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.layers import MultiHeadAttention,TimeDistributed\n",
    "from tensorflow.keras.layers import Input, MultiHeadAttention, LSTM, Dense, Flatten, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Flatten, Concatenate, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling1D\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4d7a0b-f42f-4e99-9be7-4195e8379782",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:32.703659Z",
     "iopub.status.busy": "2024-08-08T05:52:32.703244Z",
     "iopub.status.idle": "2024-08-08T05:52:39.179892Z",
     "shell.execute_reply": "2024-08-08T05:52:39.179295Z",
     "shell.execute_reply.started": "2024-08-08T05:52:32.703637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('ethylene_methane.csv')\n",
    "features = data[['TGS2602-1', 'TGS2602-2', 'TGS2600-1', 'TGS2600-2', \n",
    "                 'TGS2610-1', 'TGS2610-2', 'TGS2620-1', 'TGS2620-2',\n",
    "                 'TGS2602-3', 'TGS2602-4', 'TGS2600-3', 'TGS2600-4', \n",
    "                 'TGS2610-3', 'TGS2610-4', 'TGS2620-3', 'TGS2620-4']]\n",
    "targets = data[['Methane conc (ppm)']]\n",
    "# 划分训练集和测试集，测试集占比20%\n",
    "#X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42,shuffle=False)\n",
    "# 从X_train和y_train中划分出验证集，验证集占比25%（即训练集的1/4）\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42,shuffle=False)\n",
    "# 划分训练集和测试集，测试集占比20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "# 从X_train和y_train中划分出验证集，验证集占比25%（即训练集的1/4）\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382b1553-6fe2-457f-951a-90046a54184b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:39.181169Z",
     "iopub.status.busy": "2024-08-08T05:52:39.180692Z",
     "iopub.status.idle": "2024-08-08T05:52:39.656605Z",
     "shell.execute_reply": "2024-08-08T05:52:39.656040Z",
     "shell.execute_reply.started": "2024-08-08T05:52:39.181147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc15ae09-e38a-4bb3-9db0-b910dfad9925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:39.658358Z",
     "iopub.status.busy": "2024-08-08T05:52:39.658079Z",
     "iopub.status.idle": "2024-08-08T05:52:39.662762Z",
     "shell.execute_reply": "2024-08-08T05:52:39.662242Z",
     "shell.execute_reply.started": "2024-08-08T05:52:39.658337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, strides=1, activation='relu'):\n",
    "    \"\"\"定义一个残差块\"\"\"\n",
    "    # 保存输入以用于残差连接\n",
    "    residual = x\n",
    "\n",
    "    # 卷积层1\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "\n",
    "    # 卷积层2\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 残差连接\n",
    "    x = add([x, residual])  # 将原始输入添加到第二个卷积层的输出\n",
    "    x = Activation(activation)(x)  # 非线性激活函数\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c0fcb-ad38-46b9-9fe3-08b38bedaecb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:39.663614Z",
     "iopub.status.busy": "2024-08-08T05:52:39.663426Z",
     "iopub.status.idle": "2024-08-08T05:52:42.460686Z",
     "shell.execute_reply": "2024-08-08T05:52:42.460132Z",
     "shell.execute_reply.started": "2024-08-08T05:52:39.663595Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 13:52:39.712426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:39.713317: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:39.713488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:39.714528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:39.714696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:39.714818: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:42.089346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:42.089525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:42.089637: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 13:52:42.089738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20197 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:00:08.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_dual_stream_network(input_shape, num_heads=8, d_model=512):\n",
    "    # 定义多头注意力层\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "\n",
    "    # 第一个流\n",
    "    input1 = Input(shape=input_shape)\n",
    "    x1 = Conv1D(filters=256, kernel_size=5, activation='relu')(input1)\n",
    "    x1 = MaxPooling1D(pool_size=2)(x1)\n",
    "    x1 = Conv1D(filters=256, kernel_size=5, activation='relu')(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2)(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Flatten()(x1)\n",
    "    x1 = Dense(d_model, activation='relu')(x1)  # 调整维度以匹配多头注意力层\n",
    "\n",
    "    x1 = Reshape((1, d_model))(x1)\n",
    "\n",
    "# 添加多头注意力层\n",
    "    x1 = MultiHeadAttention(num_heads=8, key_dim=d_model)(x1, x1)\n",
    "\n",
    "    # 第二个流\n",
    "    input2 = Input(shape=input_shape)\n",
    "    x2 = Conv1D(filters=256, kernel_size=3, activation='relu')(input2)\n",
    "    x2 = MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = Conv1D(filters=256, kernel_size=3, activation='relu')(x2)\n",
    "    x2 = MaxPooling1D(pool_size=2)(x2)\n",
    "    x2 = residual_block(x2, filters=256)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    x2 = Dense(d_model, activation='relu')(x2)  # 调整维度以匹配多头注意力层\n",
    "\n",
    "    x2 = Reshape((1, d_model))(x2)\n",
    "\n",
    "# 添加多头注意力层\n",
    "    x2 = MultiHeadAttention(num_heads=8, key_dim=d_model)(x2, x2)\n",
    "\n",
    "    # 合并特征\n",
    "    merged = Concatenate()([x1, x2])\n",
    "\n",
    "    # 全连接层\n",
    "    x = Dense(512, activation='relu')(merged)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output = Dense(1, activation=None)(x)\n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    return model\n",
    "\n",
    "# 使用构建的函数创建模型\n",
    "model = build_dual_stream_network((16,1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcc9275-06a1-42b4-8eda-6e38ce494cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:42.461705Z",
     "iopub.status.busy": "2024-08-08T05:52:42.461439Z",
     "iopub.status.idle": "2024-08-08T05:52:42.504152Z",
     "shell.execute_reply": "2024-08-08T05:52:42.503678Z",
     "shell.execute_reply.started": "2024-08-08T05:52:42.461683Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │                   │            │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,401,408</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,401,408</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,864\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,864\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m1,536\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m196,864\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,024\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m327,936\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │                   │            │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,024\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │      \u001b[38;5;34m1,024\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m262,656\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │  \u001b[38;5;34m8,401,408\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │  \u001b[38;5;34m8,401,408\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │    \u001b[38;5;34m524,800\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │    \u001b[38;5;34m131,328\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │        \u001b[38;5;34m257\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,778,625</span> (71.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,778,625\u001b[0m (71.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,776,577</span> (71.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,776,577\u001b[0m (71.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af05e2a-a78b-46f4-a790-bc19f7ee516d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T05:52:42.505157Z",
     "iopub.status.busy": "2024-08-08T05:52:42.504973Z",
     "iopub.status.idle": "2024-08-08T07:14:41.557384Z",
     "shell.execute_reply": "2024-08-08T07:14:41.556816Z",
     "shell.execute_reply.started": "2024-08-08T05:52:42.505138Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723096367.390279     770 service.cc:145] XLA service 0x7f43a0020020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723096367.390308     770 service.cc:153]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6\n",
      "2024-08-08 13:52:47.506644: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-08 13:52:49.306372: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723096369.894492     804 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_52', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096371.512032     801 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096372.570546     802 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 412 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096373.609197     801 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096375.407855     807 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096375.814438     804 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_54', 176 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096376.120539     803 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096377.387687     805 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096378.451344     803 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_54', 404 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096379.548645     804 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 172 bytes spill stores, 172 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096381.152039     805 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096381.549464     803 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 416 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096382.194442     800 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_36', 416 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096382.675327     804 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_36', 152 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096382.847196     802 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 468 bytes spill stores, 696 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096383.236223     803 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_36', 444 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096384.778460     804 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_40', 468 bytes spill stores, 696 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096385.330428     801 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_40', 416 bytes spill stores, 420 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  16/2449\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - loss: 8104.8628"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723096389.724575     770 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2447/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1603.2451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723096416.194361    1046 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_54', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096416.942075    1048 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 468 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096417.163453    1051 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_54', 224 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096417.553437    1047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096418.749369    1050 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096420.405410    1047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096420.633288    1044 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 688 bytes spill stores, 676 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096421.117450    1045 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096421.441300    1048 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_52', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096421.699151    1047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 392 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096422.933256    1047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 892 bytes spill stores, 892 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096423.669070    1050 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_76', 460 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096423.942442    1044 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096424.672503    1047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_52', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096425.217880    1051 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_78', 396 bytes spill stores, 384 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096426.138417    1044 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_52', 100 bytes spill stores, 52 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1602.9280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "I0000 00:00:1723096432.551426    1219 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096433.214434    1220 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096433.570437    1223 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot', 172 bytes spill stores, 172 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096434.329614    1225 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_2', 172 bytes spill stores, 172 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096438.829825    1281 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_20', 416 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "I0000 00:00:1723096438.836438    1282 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 420 bytes spill stores, 252 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 21ms/step - loss: 1602.7697 - val_loss: 1503.3687 - learning_rate: 0.0010\n",
      "Epoch 2/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 877.3492 - val_loss: 863.2471 - learning_rate: 0.0010\n",
      "Epoch 3/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 721.5901 - val_loss: 609.2978 - learning_rate: 0.0010\n",
      "Epoch 4/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 637.4713 - val_loss: 621.7278 - learning_rate: 0.0010\n",
      "Epoch 5/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 591.7607 - val_loss: 642.0242 - learning_rate: 0.0010\n",
      "Epoch 6/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 567.0847\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 567.0692 - val_loss: 616.5069 - learning_rate: 0.0010\n",
      "Epoch 7/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 514.4641 - val_loss: 528.2804 - learning_rate: 8.0000e-04\n",
      "Epoch 8/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 497.9621 - val_loss: 775.3670 - learning_rate: 8.0000e-04\n",
      "Epoch 9/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 485.2201 - val_loss: 571.7233 - learning_rate: 8.0000e-04\n",
      "Epoch 10/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 481.0991\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 481.0853 - val_loss: 553.4688 - learning_rate: 8.0000e-04\n",
      "Epoch 11/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 444.5654 - val_loss: 470.1409 - learning_rate: 6.4000e-04\n",
      "Epoch 12/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 439.7133 - val_loss: 691.7021 - learning_rate: 6.4000e-04\n",
      "Epoch 13/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 430.4372 - val_loss: 498.3305 - learning_rate: 6.4000e-04\n",
      "Epoch 14/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 428.6818 - val_loss: 461.7121 - learning_rate: 6.4000e-04\n",
      "Epoch 15/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 419.1112 - val_loss: 454.4745 - learning_rate: 6.4000e-04\n",
      "Epoch 16/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 413.7878 - val_loss: 709.1739 - learning_rate: 6.4000e-04\n",
      "Epoch 17/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 411.2899 - val_loss: 560.1542 - learning_rate: 6.4000e-04\n",
      "Epoch 18/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 408.4566\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 408.4550 - val_loss: 538.9709 - learning_rate: 6.4000e-04\n",
      "Epoch 19/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 388.8973 - val_loss: 436.5586 - learning_rate: 5.1200e-04\n",
      "Epoch 20/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 385.4484 - val_loss: 382.7613 - learning_rate: 5.1200e-04\n",
      "Epoch 21/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 383.7705 - val_loss: 543.2123 - learning_rate: 5.1200e-04\n",
      "Epoch 22/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 380.2842 - val_loss: 381.7947 - learning_rate: 5.1200e-04\n",
      "Epoch 23/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 381.4787 - val_loss: 367.8681 - learning_rate: 5.1200e-04\n",
      "Epoch 24/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 374.4560 - val_loss: 431.4814 - learning_rate: 5.1200e-04\n",
      "Epoch 25/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 374.7473 - val_loss: 674.0057 - learning_rate: 5.1200e-04\n",
      "Epoch 26/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 372.4072\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 372.4070 - val_loss: 454.4931 - learning_rate: 5.1200e-04\n",
      "Epoch 27/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 361.2493 - val_loss: 424.3167 - learning_rate: 4.0960e-04\n",
      "Epoch 28/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 353.2749 - val_loss: 433.1671 - learning_rate: 4.0960e-04\n",
      "Epoch 29/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 355.1232 - val_loss: 355.9903 - learning_rate: 4.0960e-04\n",
      "Epoch 30/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 351.1507 - val_loss: 553.3461 - learning_rate: 4.0960e-04\n",
      "Epoch 31/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 353.1291 - val_loss: 396.3516 - learning_rate: 4.0960e-04\n",
      "Epoch 32/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 350.4851\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 350.4854 - val_loss: 476.7573 - learning_rate: 4.0960e-04\n",
      "Epoch 33/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 339.5259 - val_loss: 437.0310 - learning_rate: 3.2768e-04\n",
      "Epoch 34/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 338.2628 - val_loss: 479.1140 - learning_rate: 3.2768e-04\n",
      "Epoch 35/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 333.1616\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 333.1661 - val_loss: 415.1994 - learning_rate: 3.2768e-04\n",
      "Epoch 36/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 329.5796 - val_loss: 447.0775 - learning_rate: 2.6214e-04\n",
      "Epoch 37/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - loss: 326.4406 - val_loss: 325.6330 - learning_rate: 2.6214e-04\n",
      "Epoch 38/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 327.0772 - val_loss: 365.3110 - learning_rate: 2.6214e-04\n",
      "Epoch 39/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 320.9218 - val_loss: 315.6986 - learning_rate: 2.6214e-04\n",
      "Epoch 40/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 318.1916 - val_loss: 324.1208 - learning_rate: 2.6214e-04\n",
      "Epoch 41/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 318.6829 - val_loss: 348.9593 - learning_rate: 2.6214e-04\n",
      "Epoch 42/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 319.5453 - val_loss: 314.9491 - learning_rate: 2.6214e-04\n",
      "Epoch 43/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 317.5871 - val_loss: 330.8046 - learning_rate: 2.6214e-04\n",
      "Epoch 44/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 316.4657 - val_loss: 330.9623 - learning_rate: 2.6214e-04\n",
      "Epoch 45/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 316.3556\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 316.3577 - val_loss: 321.7995 - learning_rate: 2.6214e-04\n",
      "Epoch 46/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 309.0010 - val_loss: 318.7170 - learning_rate: 2.0972e-04\n",
      "Epoch 47/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 306.8224 - val_loss: 316.3148 - learning_rate: 2.0972e-04\n",
      "Epoch 48/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 306.9774\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 306.9788 - val_loss: 368.7101 - learning_rate: 2.0972e-04\n",
      "Epoch 49/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 297.0471 - val_loss: 331.1011 - learning_rate: 1.6777e-04\n",
      "Epoch 50/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 300.1844 - val_loss: 310.0115 - learning_rate: 1.6777e-04\n",
      "Epoch 51/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 297.5474 - val_loss: 301.1193 - learning_rate: 1.6777e-04\n",
      "Epoch 52/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 298.2044 - val_loss: 334.6236 - learning_rate: 1.6777e-04\n",
      "Epoch 53/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 295.7252 - val_loss: 346.7092 - learning_rate: 1.6777e-04\n",
      "Epoch 54/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 295.7469\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 295.7449 - val_loss: 304.2305 - learning_rate: 1.6777e-04\n",
      "Epoch 55/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 291.0481 - val_loss: 288.5670 - learning_rate: 1.3422e-04\n",
      "Epoch 56/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 288.1451 - val_loss: 350.0364 - learning_rate: 1.3422e-04\n",
      "Epoch 57/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 287.6171 - val_loss: 391.9095 - learning_rate: 1.3422e-04\n",
      "Epoch 58/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 288.0975\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 288.0962 - val_loss: 304.8340 - learning_rate: 1.3422e-04\n",
      "Epoch 59/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 281.4919 - val_loss: 349.4343 - learning_rate: 1.0737e-04\n",
      "Epoch 60/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 280.9177 - val_loss: 343.3789 - learning_rate: 1.0737e-04\n",
      "Epoch 61/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 280.6659\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 280.6664 - val_loss: 300.1219 - learning_rate: 1.0737e-04\n",
      "Epoch 62/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 273.5049 - val_loss: 287.2252 - learning_rate: 8.5899e-05\n",
      "Epoch 63/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 272.0330 - val_loss: 295.2924 - learning_rate: 8.5899e-05\n",
      "Epoch 64/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 275.5877 - val_loss: 290.2841 - learning_rate: 8.5899e-05\n",
      "Epoch 65/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 273.5546\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 273.5539 - val_loss: 297.3714 - learning_rate: 8.5899e-05\n",
      "Epoch 66/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 269.9650 - val_loss: 286.1006 - learning_rate: 6.8719e-05\n",
      "Epoch 67/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 270.4401 - val_loss: 280.8062 - learning_rate: 6.8719e-05\n",
      "Epoch 68/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 268.0878 - val_loss: 308.5094 - learning_rate: 6.8719e-05\n",
      "Epoch 69/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 268.4188 - val_loss: 326.4753 - learning_rate: 6.8719e-05\n",
      "Epoch 70/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 269.2197 - val_loss: 276.6660 - learning_rate: 6.8719e-05\n",
      "Epoch 71/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 265.7243 - val_loss: 282.0044 - learning_rate: 6.8719e-05\n",
      "Epoch 72/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 266.1142 - val_loss: 299.9646 - learning_rate: 6.8719e-05\n",
      "Epoch 73/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 265.5299\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 265.5305 - val_loss: 287.7858 - learning_rate: 6.8719e-05\n",
      "Epoch 74/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 263.0952 - val_loss: 298.3628 - learning_rate: 5.4976e-05\n",
      "Epoch 75/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 261.4966 - val_loss: 279.7769 - learning_rate: 5.4976e-05\n",
      "Epoch 76/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 261.2938\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 261.2949 - val_loss: 278.7292 - learning_rate: 5.4976e-05\n",
      "Epoch 77/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 258.5716 - val_loss: 280.4678 - learning_rate: 4.3980e-05\n",
      "Epoch 78/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 258.7078 - val_loss: 278.7347 - learning_rate: 4.3980e-05\n",
      "Epoch 79/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 254.9236\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 254.9274 - val_loss: 282.8452 - learning_rate: 4.3980e-05\n",
      "Epoch 80/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 252.4340 - val_loss: 272.1010 - learning_rate: 3.5184e-05\n",
      "Epoch 81/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 255.5044 - val_loss: 277.8956 - learning_rate: 3.5184e-05\n",
      "Epoch 82/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 255.7024 - val_loss: 275.8716 - learning_rate: 3.5184e-05\n",
      "Epoch 83/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 252.3881\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 252.3906 - val_loss: 274.0260 - learning_rate: 3.5184e-05\n",
      "Epoch 84/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 253.5794 - val_loss: 271.5343 - learning_rate: 2.8147e-05\n",
      "Epoch 85/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 252.0724 - val_loss: 272.3837 - learning_rate: 2.8147e-05\n",
      "Epoch 86/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 250.8149 - val_loss: 272.8263 - learning_rate: 2.8147e-05\n",
      "Epoch 87/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 252.1765\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 252.1738 - val_loss: 276.6906 - learning_rate: 2.8147e-05\n",
      "Epoch 88/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 247.6403 - val_loss: 270.1801 - learning_rate: 2.2518e-05\n",
      "Epoch 89/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 246.6450 - val_loss: 271.3443 - learning_rate: 2.2518e-05\n",
      "Epoch 90/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 249.2997 - val_loss: 276.9521 - learning_rate: 2.2518e-05\n",
      "Epoch 91/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 248.9168\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 248.9164 - val_loss: 274.6004 - learning_rate: 2.2518e-05\n",
      "Epoch 92/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 247.8221 - val_loss: 269.2265 - learning_rate: 1.8014e-05\n",
      "Epoch 93/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 244.7578 - val_loss: 271.5616 - learning_rate: 1.8014e-05\n",
      "Epoch 94/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 245.4955 - val_loss: 270.6996 - learning_rate: 1.8014e-05\n",
      "Epoch 95/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 245.6287\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 245.6287 - val_loss: 271.7234 - learning_rate: 1.8014e-05\n",
      "Epoch 96/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 244.9502 - val_loss: 270.0078 - learning_rate: 1.4412e-05\n",
      "Epoch 97/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 245.3727 - val_loss: 270.1620 - learning_rate: 1.4412e-05\n",
      "Epoch 98/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 243.3873\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 243.3893 - val_loss: 272.1541 - learning_rate: 1.4412e-05\n",
      "Epoch 99/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 243.6017 - val_loss: 268.0318 - learning_rate: 1.1529e-05\n",
      "Epoch 100/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 242.3047 - val_loss: 266.7853 - learning_rate: 1.1529e-05\n",
      "Epoch 101/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 242.6522 - val_loss: 268.5199 - learning_rate: 1.1529e-05\n",
      "Epoch 102/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 241.7783 - val_loss: 270.7070 - learning_rate: 1.1529e-05\n",
      "Epoch 103/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 242.6059\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 9.223372762789951e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 242.6067 - val_loss: 267.8630 - learning_rate: 1.1529e-05\n",
      "Epoch 104/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 244.5672 - val_loss: 266.4232 - learning_rate: 9.2234e-06\n",
      "Epoch 105/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 241.6258 - val_loss: 269.4184 - learning_rate: 9.2234e-06\n",
      "Epoch 106/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.6135 - val_loss: 271.1078 - learning_rate: 9.2234e-06\n",
      "Epoch 107/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 240.5760\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 7.378698501270265e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.5780 - val_loss: 267.6700 - learning_rate: 9.2234e-06\n",
      "Epoch 108/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 243.6673 - val_loss: 267.9342 - learning_rate: 7.3787e-06\n",
      "Epoch 109/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.2378 - val_loss: 267.4029 - learning_rate: 7.3787e-06\n",
      "Epoch 110/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 239.0703\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 5.902958946535364e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.0741 - val_loss: 267.0881 - learning_rate: 7.3787e-06\n",
      "Epoch 111/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.8768 - val_loss: 266.9121 - learning_rate: 5.9030e-06\n",
      "Epoch 112/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 241.6861 - val_loss: 267.1025 - learning_rate: 5.9030e-06\n",
      "Epoch 113/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 241.8978\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.7223671572282915e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 241.8957 - val_loss: 267.8317 - learning_rate: 5.9030e-06\n",
      "Epoch 114/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.2508 - val_loss: 266.3992 - learning_rate: 4.7224e-06\n",
      "Epoch 115/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.6468 - val_loss: 266.4682 - learning_rate: 4.7224e-06\n",
      "Epoch 116/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.5607 - val_loss: 266.9281 - learning_rate: 4.7224e-06\n",
      "Epoch 117/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 239.2223\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.7778936530230567e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.2227 - val_loss: 269.5148 - learning_rate: 4.7224e-06\n",
      "Epoch 118/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.0458 - val_loss: 267.4945 - learning_rate: 3.7779e-06\n",
      "Epoch 119/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.9536 - val_loss: 265.9539 - learning_rate: 3.7779e-06\n",
      "Epoch 120/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.4835 - val_loss: 266.0219 - learning_rate: 3.7779e-06\n",
      "Epoch 121/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.7705 - val_loss: 266.1658 - learning_rate: 3.7779e-06\n",
      "Epoch 122/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 238.9666\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.0223149224184457e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.9668 - val_loss: 266.4687 - learning_rate: 3.7779e-06\n",
      "Epoch 123/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - loss: 238.8679 - val_loss: 265.8447 - learning_rate: 3.0223e-06\n",
      "Epoch 124/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.0083 - val_loss: 266.8314 - learning_rate: 3.0223e-06\n",
      "Epoch 125/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.9922 - val_loss: 266.4358 - learning_rate: 3.0223e-06\n",
      "Epoch 126/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.0638\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 2.4178520106943328e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.0665 - val_loss: 266.0098 - learning_rate: 3.0223e-06\n",
      "Epoch 127/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.8656 - val_loss: 266.5096 - learning_rate: 2.4179e-06\n",
      "Epoch 128/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.9407 - val_loss: 266.3430 - learning_rate: 2.4179e-06\n",
      "Epoch 129/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.3816\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 1.9342816813150422e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.3835 - val_loss: 266.4593 - learning_rate: 2.4179e-06\n",
      "Epoch 130/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.5008 - val_loss: 265.8419 - learning_rate: 1.9343e-06\n",
      "Epoch 131/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.8237 - val_loss: 265.8433 - learning_rate: 1.9343e-06\n",
      "Epoch 132/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.9479 - val_loss: 265.9688 - learning_rate: 1.9343e-06\n",
      "Epoch 133/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.2557 - val_loss: 265.6858 - learning_rate: 1.9343e-06\n",
      "Epoch 134/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.6181 - val_loss: 265.8221 - learning_rate: 1.9343e-06\n",
      "Epoch 135/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.7628 - val_loss: 266.1563 - learning_rate: 1.9343e-06\n",
      "Epoch 136/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 235.3707\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 1.547425381431822e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.3748 - val_loss: 265.8020 - learning_rate: 1.9343e-06\n",
      "Epoch 137/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.6163 - val_loss: 265.7355 - learning_rate: 1.5474e-06\n",
      "Epoch 138/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.3171 - val_loss: 265.8139 - learning_rate: 1.5474e-06\n",
      "Epoch 139/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 239.5836\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 1.2379403415252455e-06.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.5816 - val_loss: 265.8253 - learning_rate: 1.5474e-06\n",
      "Epoch 140/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.0302 - val_loss: 265.6992 - learning_rate: 1.2379e-06\n",
      "Epoch 141/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.9836 - val_loss: 265.9053 - learning_rate: 1.2379e-06\n",
      "Epoch 142/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 240.6907\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 9.903522368404083e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.6852 - val_loss: 265.8574 - learning_rate: 1.2379e-06\n",
      "Epoch 143/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.0905 - val_loss: 265.5476 - learning_rate: 9.9035e-07\n",
      "Epoch 144/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.4158 - val_loss: 266.2202 - learning_rate: 9.9035e-07\n",
      "Epoch 145/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.0133 - val_loss: 265.7365 - learning_rate: 9.9035e-07\n",
      "Epoch 146/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.4158\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 7.922817530925386e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.4153 - val_loss: 265.6336 - learning_rate: 9.9035e-07\n",
      "Epoch 147/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 234.5664 - val_loss: 265.7720 - learning_rate: 7.9228e-07\n",
      "Epoch 148/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.4324 - val_loss: 265.6838 - learning_rate: 7.9228e-07\n",
      "Epoch 149/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 238.1714\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 6.338254024740309e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.1698 - val_loss: 265.6364 - learning_rate: 7.9228e-07\n",
      "Epoch 150/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.0042 - val_loss: 265.8141 - learning_rate: 6.3383e-07\n",
      "Epoch 151/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.2232 - val_loss: 265.7608 - learning_rate: 6.3383e-07\n",
      "Epoch 152/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 238.4120\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 5.070603037893307e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.4101 - val_loss: 265.8676 - learning_rate: 6.3383e-07\n",
      "Epoch 153/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 240.1106 - val_loss: 265.7052 - learning_rate: 5.0706e-07\n",
      "Epoch 154/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.7888 - val_loss: 265.7839 - learning_rate: 5.0706e-07\n",
      "Epoch 155/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 236.2036\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 4.056482339365175e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.2048 - val_loss: 265.6905 - learning_rate: 5.0706e-07\n",
      "Epoch 156/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.1882 - val_loss: 265.6300 - learning_rate: 4.0565e-07\n",
      "Epoch 157/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.0837 - val_loss: 265.8047 - learning_rate: 4.0565e-07\n",
      "Epoch 158/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 235.6151\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.24518578054267e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.6172 - val_loss: 265.6936 - learning_rate: 4.0565e-07\n",
      "Epoch 159/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 239.3268 - val_loss: 265.5667 - learning_rate: 3.2452e-07\n",
      "Epoch 160/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.4364 - val_loss: 265.5999 - learning_rate: 3.2452e-07\n",
      "Epoch 161/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 236.3331\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.5961485334846656e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.3335 - val_loss: 265.7916 - learning_rate: 3.2452e-07\n",
      "Epoch 162/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.3865 - val_loss: 265.6541 - learning_rate: 2.5961e-07\n",
      "Epoch 163/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.8290 - val_loss: 265.7620 - learning_rate: 2.5961e-07\n",
      "Epoch 164/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.3889\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.076918917737203e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.3889 - val_loss: 265.8186 - learning_rate: 2.5961e-07\n",
      "Epoch 165/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 238.1164 - val_loss: 265.7476 - learning_rate: 2.0769e-07\n",
      "Epoch 166/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.6333 - val_loss: 265.6510 - learning_rate: 2.0769e-07\n",
      "Epoch 167/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 235.9094\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 1.6615351796644973e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 235.9116 - val_loss: 265.6971 - learning_rate: 2.0769e-07\n",
      "Epoch 168/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 234.4121 - val_loss: 265.8170 - learning_rate: 1.6615e-07\n",
      "Epoch 169/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.6317 - val_loss: 265.7454 - learning_rate: 1.6615e-07\n",
      "Epoch 170/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 237.9626\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 1.329228098256863e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 237.9611 - val_loss: 265.7525 - learning_rate: 1.6615e-07\n",
      "Epoch 171/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.0083 - val_loss: 265.7964 - learning_rate: 1.3292e-07\n",
      "Epoch 172/5000\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.1313 - val_loss: 265.6554 - learning_rate: 1.3292e-07\n",
      "Epoch 173/5000\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 236.6674\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 1.0633824558681227e-07.\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - loss: 236.6680 - val_loss: 265.6036 - learning_rate: 1.3292e-07\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, min_lr=0.0000001, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    [X_train_scaled,X_train_scaled], y_train,\n",
    "    epochs=5000,\n",
    "    batch_size=1024,\n",
    "    validation_data=([X_val_scaled,X_val_scaled], y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],  # 添加回调函数\n",
    "    #callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3f8231-6506-4376-b48f-0343df674736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T07:14:41.558468Z",
     "iopub.status.busy": "2024-08-08T07:14:41.558252Z",
     "iopub.status.idle": "2024-08-08T07:14:41.716248Z",
     "shell.execute_reply": "2024-08-08T07:14:41.715723Z",
     "shell.execute_reply.started": "2024-08-08T07:14:41.558447Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a99fa9-b704-466f-bf83-a76ac2a04f2a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T07:14:41.717505Z",
     "iopub.status.busy": "2024-08-08T07:14:41.717034Z",
     "iopub.status.idle": "2024-08-08T07:14:41.940301Z",
     "shell.execute_reply": "2024-08-08T07:14:41.939686Z",
     "shell.execute_reply.started": "2024-08-08T07:14:41.717484Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('dual-cnn-att-residuals-large.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5152eb69-de69-4e42-bb16-346a3bd7cd40",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T07:14:41.942470Z",
     "iopub.status.busy": "2024-08-08T07:14:41.942073Z",
     "iopub.status.idle": "2024-08-08T07:14:42.113601Z",
     "shell.execute_reply": "2024-08-08T07:14:42.113067Z",
     "shell.execute_reply.started": "2024-08-08T07:14:41.942443Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('dual-cnn-att-residuals-large.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3400fb6-74ff-444b-9dad-a2c29928c29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T07:14:42.115111Z",
     "iopub.status.busy": "2024-08-08T07:14:42.114555Z",
     "iopub.status.idle": "2024-08-08T07:15:32.960510Z",
     "shell.execute_reply": "2024-08-08T07:15:32.959906Z",
     "shell.execute_reply.started": "2024-08-08T07:14:42.115078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26116/26116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([X_test_scaled,X_test_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d32673-a290-4284-9cff-828f6e00af7c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T07:15:32.961761Z",
     "iopub.status.busy": "2024-08-08T07:15:32.961552Z",
     "iopub.status.idle": "2024-08-08T07:15:32.964761Z",
     "shell.execute_reply": "2024-08-08T07:15:32.964189Z",
     "shell.execute_reply.started": "2024-08-08T07:15:32.961741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped_array = predictions.reshape(835701, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ecd7f0d-2254-4fce-b958-a12c2f23d94c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-08T07:15:32.965777Z",
     "iopub.status.busy": "2024-08-08T07:15:32.965450Z",
     "iopub.status.idle": "2024-08-08T07:15:32.981043Z",
     "shell.execute_reply": "2024-08-08T07:15:32.980550Z",
     "shell.execute_reply.started": "2024-08-08T07:15:32.965756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 267.24236716454226\n",
      "Test RMSE: 16.347549270901197\n",
      "Test MAE: 3.5425449446879007\n",
      "R2 Score: 0.9545512199401855\n"
     ]
    }
   ],
   "source": [
    "# 计算MSE和RMSE\n",
    "mse = mean_squared_error(y_test, reshaped_array)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# 计算MAE\n",
    "mae = mean_absolute_error(y_test, reshaped_array)\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print('R2 Score:', r2_score(y_test, reshaped_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94990cda-e261-4423-aa5f-3ac4edba1c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T07:15:32.982029Z",
     "iopub.status.busy": "2024-08-08T07:15:32.981845Z",
     "iopub.status.idle": "2024-08-08T07:16:13.817147Z",
     "shell.execute_reply": "2024-08-08T07:16:13.816598Z",
     "shell.execute_reply.started": "2024-08-08T07:15:32.982010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26116/26116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1ms/step\n",
      "Total inference (prediction) time: 40.83 seconds\n",
      "Average inference (prediction) time per sample: 0.00005 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 推理（预测）时间\n",
    "start_predict_time = time.time()\n",
    "\n",
    "predictions = model.predict([X_test_scaled, X_test_scaled])\n",
    "\n",
    "# 计算总推理时间\n",
    "end_predict_time = time.time()\n",
    "total_predict_time = end_predict_time - start_predict_time\n",
    "print(f\"Total inference (prediction) time: {total_predict_time:.2f} seconds\")\n",
    "\n",
    "# 计算平均推理时间\n",
    "num_predictions = len(predictions) \n",
    "average_predict_time = total_predict_time / num_predictions\n",
    "print(f\"Average inference (prediction) time per sample: {average_predict_time:.5f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
